# ğŸš€ Spark-Airflow-Postgres

Projekt pro automatizaci a orchestraci ETL procesÅ¯ pomocÃ­ Apache Airflow, Apache Spark a PostgreSQL.

## ğŸ”¥ PÅ™ehled

Tento projekt implementuje datovou pipeline, kterÃ¡ automatizuje zpracovÃ¡nÃ­ dat pomocÃ­ Apache Airflow, transformace provÃ¡dÃ­ v Apache Spark a uklÃ¡dÃ¡ vÃ½sledky do PostgreSQL databÃ¡ze.

## ğŸš€ Funkce

- ğŸŒŠ Automatizace a orchestrace ETL procesÅ¯ pomocÃ­ Apache Airflow
- âš¡ ZpracovÃ¡nÃ­ a transformace dat s vyuÅ¾itÃ­m Apache Spark
- ğŸ˜ UklÃ¡dÃ¡nÃ­ a pÅ™Ã­stup k datÅ¯m pÅ™es PostgreSQL
- ğŸ“Š SprÃ¡va databÃ¡ze pomocÃ­ pgAdmin

## ğŸ“Š Architektura

Projekt vyuÅ¾Ã­vÃ¡ nÃ¡sledujÃ­cÃ­ komponenty:

1. **Apache Airflow** - Orchestrace a plÃ¡novÃ¡nÃ­ ETL procesÅ¯
2. **Apache Spark** - ZpracovÃ¡nÃ­ a transformace dat
3. **PostgreSQL** - PersistentnÃ­ uloÅ¾iÅ¡tÄ› pro vÃ½slednÃ¡ data
4. **pgAdmin** - WebovÃ© rozhranÃ­ pro sprÃ¡vu PostgreSQL

## âš™ï¸ Instalace a spuÅ¡tÄ›nÃ­

### PoÅ¾adavky

- Docker a Docker Compose
- Git

### RychlÃ© spuÅ¡tÄ›nÃ­

```bash
# KlonovÃ¡nÃ­ repozitÃ¡Å™e
git clone https://github.com/msm00/spark-airflow-postgres.git
cd spark-airflow-postgres

# KopÃ­rovÃ¡nÃ­ konfiguraÄnÃ­ho souboru
cp .env.example .env

# SpuÅ¡tÄ›nÃ­ sluÅ¾eb
docker-compose up -d
```

### PÅ™Ã­stup k webovÃ½m rozhranÃ­m

- **Airflow**: http://localhost:8080
  - UÅ¾ivatel: airflow
  - Heslo: airflow
- **pgAdmin**: http://localhost:5050
  - Email: admin@example.com
  - Heslo: admin
- **Spark UI**: http://localhost:8090 (dostupnÃ© bÄ›hem bÄ›hu Spark Ãºlohy)

## ğŸ“‹ PouÅ¾itÃ­

### SpuÅ¡tÄ›nÃ­ ETL procesu

ETL procesy jsou definovÃ¡ny jako Airflow DAGy v adresÃ¡Å™i `airflow/dags`. Proces mÅ¯Å¾ete spustit manuÃ¡lnÄ› z Airflow UI nebo se spustÃ­ automaticky podle nastavenÃ©ho rozvrhu.

### VytvoÅ™enÃ­ Spark jobu

1. PÅ™idejte novÃ½ Python soubor do adresÃ¡Å™e `spark_jobs/`
2. Implementujte transformaÄnÃ­ logiku
3. Integrujte Spark job s Airflow pomocÃ­ SparkSubmitOperator

## ğŸ“ Struktura projektu

```
.
â”œâ”€â”€ airflow/                     # Apache Airflow konfigurace
â”‚   â”œâ”€â”€ dags/                    # DAG definice
â”‚   â”‚   â””â”€â”€ spark_etl_dag.py     # UkÃ¡zkovÃ½ ETL DAG
â”‚   â””â”€â”€ plugins/                 # Airflow pluginy
â”œâ”€â”€ spark_jobs/                  # Spark transformaÄnÃ­ skripty
â”‚   â””â”€â”€ etl_process.py           # UkÃ¡zkovÃ½ Spark job
â”œâ”€â”€ db-init/                     # SQL skripty pro inicializaci databÃ¡ze
â”‚   â””â”€â”€ 01_create_tables.sql     # Definice tabulek
â”œâ”€â”€ data/                        # AdresÃ¡Å™ pro vstupnÃ­ data
â”‚   â””â”€â”€ sample_data.csv          # VzorovÃ¡ data
â”œâ”€â”€ tests/                       # Testy
â”‚   â”œâ”€â”€ test_dags.py             # Testy pro Airflow DAGy
â”‚   â””â”€â”€ test_spark_jobs.py       # Testy pro Spark joby
â”œâ”€â”€ docker-compose.yml           # Konfigurace sluÅ¾eb
â”œâ”€â”€ Dockerfile                   # Definice Docker image
â”œâ”€â”€ .env.example                 # VzorovÃ¡ konfigurace prostÅ™edÃ­
â”œâ”€â”€ requirements.txt             # Python zÃ¡vislosti
â””â”€â”€ README.md                    # Tato dokumentace
```

## ğŸ§‘â€ğŸ’» VÃ½voj

### Best Practices

- KaÅ¾dÃ½ DAG musÃ­ mÃ­t dokumentaci
- Implementujte retry mechanismy
- PouÅ¾Ã­vejte connections pro sprÃ¡vu pÅ™Ã­stupu k databÃ¡zÃ­m
- Optimalizujte Spark joby (pouÅ¾Ã­vejte cache(), persist() s rozmyslem)

## ğŸ“ Licence

MIT 